Lecture: Hash Tables
====================

* Python dictionaries are hash tables

* Dictionaray Abstract Data Type
  a set of items with keys 

  insert(item) (overwrites if there is already an item with same key)
  delete(item)
  search(key): return item or report error

  in Python:
    item is a pair: (key,val)
    D[key] = val    insert
    del D[key]      delete
    D[key]          search

  Most modern languages have hash tables built-in or in the
  standard library. 

* Can implement Dictionary ADT with AVL Trees, search in O(lg n)

* Motivation: dictionaries are everywhere!
  - compilers
  - Python interpreter: variables stores in dictionaries
  - spell checking
  - search engines use to use dictionaries that linked a word to
    web pages that the word appears in
  - computer login
  - network router to lookup local machines
  - substring search, string commonoalities (DNA)

* Simple dictionary implementation
  If keys are integers, can use direct-access table (i.e, array)
  - store items in array indexed by key (draw picture)
    use None to indicate absense of key
  - what's bad?
    1. keys may not be natural numbers
    2. memory hog if set of possible keys is huge, if much
       larger thant than the number of keys store in the dictionary

* Prehashing fixes problem 1. Mapping everything to integers.
  (DSA textbook calls this the creation of a hash code.)

  In Python, hash(v) computes the prehash of v.
  Ideally: hash(x) = hash(y) iff x == y
  User-definable: __hash__, default is the address in memory

* Algorithm for prehashing a string (not Python's) (polynomial hash code)
  Map each character to one digit in a number
  But there are 256 different characters, not 10.
  So we use a different base.
  
  prehash_string('ab') == 97 * 256 + 98
  prehash_string('abc') == 97 * (256**2) + 98 * (256**1) + 99

* Hashing fixes problem 2. (DSA textbook calls this "compression")
  from cooking: "a finely chopped mixture"
  - draw picture of universe of keys getting mapped down by hash function h to 
    0...m  (for a table of size m)
  - a subset of the universe is present in the table, subset is size n
  - we want m = O(n)
  - problems with this idea? answser: collision
      h(k_i) = h(k_j)

* Chaining fixes collisions.
  - each slot of the hashtable contains a linked list of the items that
    collided (had the same hash value)
  - draw picture
  - worst case: search in O(n)

* Towards proving that the average case time is O(1)
  - Simple Uniform Hashing assumption (mostly true but not completely true)
    each key is equally likedly to be hashed to each slot of the table
    independent of where other keys land. (uniformity and idependence)

  - what's the expected length of a chain?
    n keys in m slots: n/m = alpha  (load factor)
    
  - Search:
    1. hash the key: O(1)
    2. find the chain: O(1)
    3. linear search in the chain: O(alpha)
    total: O(1 + alpha)

* hash functions
  - division method
    h(k) = k mod m
    need to be careful about choice of table size m
    if not, may not use all of the table

    table size 4 (slots 0..3)
    suppose the keys are all even: 0,2,..
    0 -> 0           (0 mod 4 = 0)
    2 -> 2           (2 mod 4 = 2)
    4 -> 0           (4 mod 4 = 0)
    6 -> 2           (6 mod 4 = 2)
    8 -> 0           (8 mod 4 = 0)
    ...
    Never use slot 1 and 3.

    Good to choose a prime number for m, not close to
    a power of 2 or 10.

  - Multiply-Add-and-Divide (MAD) method

    h(k) = ((a * k + b) mod p) mod m
      where
      p is a prime number larger than m
      a,b are randomly chosen integers between 1 and p-1.

*** student exercise: 
    using the division method and chaining, insert some
    keys into a hash table.

* Open addressing is an alternative solution to the collision problem.

* Advantage: instead of a linked list per slot, open addressing stores 
  at most one item per hashtable slot.

* When there is a collision, the insert function looks for another
  slot that is open, a process called *probing*. The locations of the
  slots are determined by the hash function, which now depends on a
  probe sequence number in addition to the key. Thus, the sequence of
  slots to look in for a particular key k are:

  h(k,0), h(k,1), h(k,2), ...

* It's important that the hash function have the property that a probe
  sequence h(k,0)...h(k,m-1) hits all of the table slots
  0...m-1. Otherwise, it would waste some slots of the table.
  (It must be a permutation of 0...m-1.)

* Table size must be larger than number of items. n <= m

* Example

  Hash table
  0: 
  1: 586 
  2: 133
  3:
  4: 204
  5: 
  6: 481
  7:

  Insert 496
  h(496,0) = 4   collision
  h(496,1) = 6   collision
  h(496,2) = 1   collisoin
  h(496,3) = 5   free!


* Python implementation

   def hash_insert(T, k):
      i = 0
      j = h(k,i)
      while T[j] != None:
        i += 1
        if i == m:
           raise HashTableOverflow
        j = h(k,i)
      T[j] = k
      return j

**** Student group work: implement hash_search(T, k).

* Discuss deletion.
  - Can't just mark slot as None because that would interfere
    with search. For example, delete 586 in the above example.
    A subsequence search for 496 returns None!
  - Idea of using a "delete me" flag.
    During search, skip over delete-me items.
    During insert, treat delete-me same as None.

* hash functions for open addressing

  Ideally the hash function would have the same
  probability of producing any of the m! permutations
  of 0...m-1. This is the *Uniform Hashing* property.
  No practical hash function acheives this,
  and some get closer to this than others.

  - Linear probing
     Given any hash function h', wrap it up as follows
     to create h:

     h(k,i) = (h'(k) + i) mod m

     Suffers from the *primary clustering* problem.
     You get long contiguous chunks of filled slots, so
     inserting and searching takes longer.

     Usually an empty slot gets selected with a probability
     of 1/m. However, if it is preceded by i full slots,
     then it's probability increases to (i+1)/m.

     There are only m distinct probe sequences, so
     this is far from providing Uniform Hashing.

  - quadratic probing

    h(k,i) = (h'(k) + c1*i + c2*i^2) mod m

    Have to choose c1,c2, and m carefully to ensure
    that the probe sequence is a permutation of 0...m-1.

    Like linear probing, there are only m distinct probe
    sequences.

    With quadratic probing you can still see mild clustering.

  - double hashing
    Uses two hash functions h1 and h2:

    h(k,i) = (h1(k) + i*h2(k)) mod m

    h2(k) needs to be relatively prime to m to ensure that
    the probe sequence is a permutation of 0...m.
    Recall that two integers are *relatively prime* if they share
    no positive factors except 1. (So their gcd is 1.)

    Here's what happens when h2(k) is not rel. prime to m:
    m = 4
    h1(k) = 5
    h2(k) = 2
    (5 + 2*0) mod 4 = 1
    (5 + 2*1) mod 4 = 3
    (5 + 2*2) mod 4 = 1
    (5 + 2*3) mod 4 = 3

    One way to do make m and h2(k) relatively prime is let m be a power
    of 2 and design h2 to produce odd integers.

    Now we're ok:
    m = 4
    h1(k) = 5
    h2(k) = 3
    (5 + 3*0) mod 4 = 1
    (5 + 3*1) mod 4 = 0
    (5 + 3*2) mod 4 = 3
    (5 + 3*3) mod 4 = 2

    Double hashing provides m^2 distinct probe sequences,
    still many fewer than m!, but a lot better than m.
    
* Average time complexity of hashtables using open addressing

  Suppose we have inserted n items into a table of size m.
  Assuming uniform hashing, the next operation has expected
  cost (# probes) of <= 1/(1-alpha) if alpha < 1.
  Recall that alpha = n/m.

  This grows as alpha tends towards 1.

  Need to keep alpha under 0.5 to keep things working well.
  1/(1-0.5) = 1/0.5 = 2 probes
  1/(1-0.9) = 1/0.1 = 10 probes
