Lecture: Time complexity
===============================================================

* Searching

  In the Flood-It! game, we don't want duplicates in the flooded_list
  (otherwise it would get too long which would slow things down), so
  before adding a coordinate to the flooded_list, we first check to
  see if it's already there.

  * sequential search (aka. linear search)

    How does Python implement the 'in' keyword?
    1 in [6,5,3,4,1,2]

    Answer:

    def sequential_search(x, A):
	for i in range(0, len(A)):
	    if A[i] == x:
		return i
	return len(A)
  
    How much time does it take for sequential_search to finish?
    * it depends: on the inputs x and A, on the computer, 
      on what other processes are running on the computer, etc.
    * on average? in the best case? worst case? 
    * as the size of A grows, how fast does the worst-case time grow?
      answer: it grows linearly, that is,

      T(n) = c * n      for some constant c

    Can we do better?

  * binary search

    Divide and conquer!

     *** Ask students to implement binary search
    interface: binary_search(x, A, start, end)

    answer:

    def binary_search(x, A, start, end, compare):
        if start == end:
            return end
        else:
            middle = start + (end - start) // 2
            if compare(x, A[middle]) < 0:
                return binary_search(x, A, start, middle, compare)
            elif compare(x, A[middle]) > 0:
                return binary_search(x, A, middle + 1, end, compare)
            else:
                return middle

    what's the time complexity?

    rec. depth    # items in array segment
    1             n/2
    2             n/4
    ...
    i             n/(2^i)

    we're done when there is 1 item left:

       n/2^i  =  1
       n = 2^i
       lg n = i

    T(n) = c * lg n

    (remind students to study section 3.2 of Goodrich et al.)

* Time Complexity In General
    * we are interested in the running time as the inputs grow large
    * so we care about the growth rate of an algorithm's run-time more than
      it's value at a particular point.
    * also, we tend to focus on the worst-cast run-time of algorithms because
      we want to offer guarantees to the user of an algorithm

    * we want to define a notion of equality between functions that
      grow at a similar rate but that ignores details that don't matter

      Functions to think about: n, 10n, lg n, n lg n, 10 n^2, n^2 + n

* Upper bounds

   Def. (Big-O) For a given function g(n), we define O(g(n)) as the
   the set of functions that grow no faster than g(n), or more
   specifically,

   O(g(n)) = { f(n) | ∃ n0. ∀ n >= n0. ∃ c. f(n) <= c g(n) }

   We say that g(n) is an *asymptotic upper bound* of all the functions
   in the set O(g(n)).

   Let's show that n^2 + n + 10 \in O(n^2)

   n^2 + n + 10 <= c2 n^2
     divide by n^2
   1 + 1/n + 10/n^2 <= c2
     choose c2 = 2

     n	    1 + 1/n + 10/n^2
     1	    1 + 1 + 10/1    = 12
     2	    1 + 1/2 + 10/4  = 4
     3	    1 + 1/3 + 10/9  = 2.44
     4	    1 + 1/4 + 10/16 = 1.875

     choose n0 = 4

   ∀ n >= 4. n^2 + n + 10 <= 2 n^2
   
     n   n^2 + n + 10	2n^2
     1   12    	   	2
     2   16		8
     3   22		18
     4   30		32
     5	 40		50
     6	 52		72


* Lower bounds

   Def. (Omega) For a given function g(n), we define Ω(g(n)) as the
   set of functions that grow at least as fast a g(n)

   Ω(g(n)) = { f(n) | ∃ n0. ∀ n >= n0. ∃ c. 0 <= c g(n) <= f(n) }

* Tight bounds

   Def. (Theta) For a given function g(n), Θ(g(n)) is the set of functions
   that grow at the same rate as g(n), or more specifically,

   Θ(g(n)) = { f(n) | 
               ∃ n0. ∀ n >= n0. ∃ c1 c2. 0 <= c1 g(n) <= f(n) <= c2 g(n) }

   We say that g(n) is an *asymptotically tight bound* for each function
   in Θ(g(n)).

   Relationships:
     * Θ(g(n)) \subseteq O(g(n))
     * Θ(g(n)) \subseteq Ω(g(n))
     * Θ(g(n)) = Ω(g(n)) \cap O(g(n))

